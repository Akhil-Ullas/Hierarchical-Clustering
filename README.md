âœ” A Short Overview of Hierarchical Clustering

Hierarchical Clustering is an unsupervised learning technique that builds clusters in the form of a tree-like structure (dendrogram), allowing data to be grouped:

â€¢ progressively (step-by-step)
â€¢ at different granularity levels
â€¢ without pre-specifying the number of clusters

Unlike partition-based methods such as K-Means, hierarchical clustering focuses on relationship structure, not just compact grouping.

ðŸ“Œ Agglomerative vs Divisive Clustering

The module primarily focused on:

ðŸ”¹ Agglomerative Clustering (Bottom-Up)
â€¢ each point starts as an individual cluster
â€¢ similar clusters are merged step-by-step
â€¢ process continues until a single cluster remains

(used in most practical applications)

ðŸ”¹ Divisive Clustering (Top-Down)
â€¢ all points start in one cluster
â€¢ recursively split into smaller clusters

This provided deeper intuition into how cluster trees evolve over time.

ðŸ“Œ Linkage Methods Explored

I experimented with different linkage strategies to understand how distance influences cluster merging:

â€¢ Single Linkage â€” nearest-neighbor merging
â€¢ Complete Linkage â€” farthest-neighbor merging
â€¢ Average Linkage â€” mean distance merging
â€¢ Wardâ€™s Method â€” variance-minimizing merging

Each linkage style produced different cluster shapes, separations, and interpretations, reinforcing the importance of selecting linkage based on data behavior rather than default settings.

âœ” Key Concepts & Insights Gained

â€¢ Role of distance metrics in clustering
â€¢ Effect of scaling & normalization on structure
â€¢ Cluster granularity vs interpretability
â€¢ Choosing cut-levels on the dendrogram
â€¢ Trade-offs between compactness & hierarchy depth

I also compared how clusters changed when:

â€¢ features were rescaled
â€¢ distance metrics were modified
â€¢ linkage strategies were altered

This strengthened my understanding of cluster stability and robustness.

ðŸ“Œ Dendrogram Interpretation

One of the most valuable outcomes was learning to interpret dendrograms to:

â€¢ identify natural breakpoints
â€¢ observe merging behavior
â€¢ understand similarity relationships
â€¢ infer hierarchical group structure

Instead of selecting a fixed K, I evaluated:

â€¢ where meaningful cluster separation emerged
â€¢ whether clusters were dense, overlapping, or fragmented

âœ” Applications Where Hierarchical Clustering Is Useful

â€¢ Customer & behavior segmentation
â€¢ Social grouping & similarity analysis
â€¢ Document or gene similarity mapping
â€¢ Hierarchical category discovery
â€¢ Feature grouping in preprocessing pipelines

It proved especially insightful when cluster relationships mattered more than labels.

âœ” Advantages Observed

â€¢ No need to pre-define number of clusters
â€¢ Provides a full hierarchy instead of a single solution
â€¢ Useful for exploratory data analysis
â€¢ Dendrograms make structure visually interpretable

âœ˜ Limitations Acknowledged

â€¢ Computationally expensive for large datasets
â€¢ Sensitive to noise and scaling
â€¢ Difficult to revise once clusters are merged
â€¢ Results vary significantly with linkage choice
